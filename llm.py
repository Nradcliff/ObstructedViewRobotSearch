# -*- coding: utf-8 -*-
"""llm.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Br1J1TX5RWAXPtCkv4zTwveJO9JNUVtV
"""

import requests
import base64

# Constants
INVOKE_URL = "https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions"
API_KEY = "YOUR_NVIDIA_API_KEY"  # Replace with your NVIDIA API key

def query_llm(image_path: str) -> str:
    """
    Sends an image to the NVIDIA LLaMA API and returns a movement command.
    """
    # Encode image to base64
    with open(image_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()

    # Ensure image size is within limits
    assert len(image_b64) < 180_000, "Image too large. Use the assets API for larger images."

    # Prepare headers and payload
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Accept": "application/json"  # Disable streaming for simplicity
    }

    payload = {
        "model": "meta/llama-3.2-11b-vision-instruct",
        "messages": [
            {
                "role": "user",
                "content": f'Based on this image, where should the robot move? '
                           f'Return a command like "left 0.5" or "forward 1.0". '
                           f'<img src="data:image/png;base64,{image_b64}" />'
            }
        ],
        "max_tokens": 512,
        "temperature": 0.7,  # Lower temperature for more deterministic output
        "top_p": 1.0,
        "stream": False  # Disable streaming for simplicity
    }

    # Send request to NVIDIA API
    response = requests.post(INVOKE_URL, headers=headers, json=payload)
    if response.status_code == 200:
        # Extract the LLM's response
        llm_response = response.json()["choices"][0]["message"]["content"]
        return llm_response.strip()  # Return the movement command
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None

# Example usage
if __name__ == "__main__":
    image_path = "image.png"  # Replace with the path to your image
    command = query_llm(image_path)
    print(f"LLM Command: {command}")